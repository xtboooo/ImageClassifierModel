"""ä¸€é”®è®­ç»ƒæµæ°´çº¿è„šæœ¬ - å®Œæ•´çš„è®­ç»ƒã€è¯„ä¼°ã€å¯¼å‡ºã€æµ‹è¯•æµç¨‹"""
import argparse
import json
import shutil
import sys
import time
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import torch


def format_time(seconds):
    """æ ¼å¼åŒ–æ—¶é—´æ˜¾ç¤º"""
    if seconds < 60:
        return f"{seconds:.1f}ç§’"
    elif seconds < 3600:
        minutes = int(seconds // 60)
        secs = int(seconds % 60)
        return f"{minutes}åˆ†{secs}ç§’"
    else:
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        return f"{hours}å°æ—¶{minutes}åˆ†"


class PipelineRunner:
    """æµæ°´çº¿æ‰§è¡Œå™¨"""

    def __init__(self, args):
        self.args = args
        self.run_dir = None
        self.config = None
        self.checkpoint_path = None
        self.results = {
            'start_time': None,
            'end_time': None,
            'stages': {},
            'errors': []
        }

    def run(self):
        """æ‰§è¡Œå®Œæ•´æµæ°´çº¿"""
        self.results['start_time'] = time.time()

        print("\n" + "="*70)
        print("ğŸš€ æ¨¡å‹è®­ç»ƒæµæ°´çº¿")
        print("="*70)

        try:
            # é˜¶æ®µ1: åˆå§‹åŒ–
            self._run_stage("åˆå§‹åŒ–", self.setup)

            # é˜¶æ®µ2: æ•°æ®å‡†å¤‡
            self._run_stage("æ•°æ®å‡†å¤‡", self.prepare_data)

            # é˜¶æ®µ3: æ¨¡å‹è®­ç»ƒ
            if not self.args.skip_train:
                self._run_stage("æ¨¡å‹è®­ç»ƒ", self.train_model)
            else:
                print("\n[é˜¶æ®µ3/8] æ¨¡å‹è®­ç»ƒ")
                print("  âŠ˜ å·²è·³è¿‡ï¼ˆä½¿ç”¨å·²æœ‰æ¨¡å‹ï¼‰")
                self.load_existing_model()

            # é˜¶æ®µ4: æ¨¡å‹è¯„ä¼°
            self._run_stage("æ¨¡å‹è¯„ä¼°", self.evaluate_model)

            # é˜¶æ®µ5: æ¨¡å‹å¯¼å‡º
            if not self.args.skip_export:
                self._run_stage("æ¨¡å‹å¯¼å‡º", self.export_models)
            else:
                print("\n[é˜¶æ®µ5/8] æ¨¡å‹å¯¼å‡º")
                print("  âŠ˜ å·²è·³è¿‡")

            # é˜¶æ®µ6: æµ‹è¯•å›¾ç‰‡æ¨ç†
            if not self.args.skip_test:
                self._run_stage("æµ‹è¯•å›¾ç‰‡æ¨ç†", self.test_inference)
            else:
                print("\n[é˜¶æ®µ6/8] æµ‹è¯•å›¾ç‰‡æ¨ç†")
                print("  âŠ˜ å·²è·³è¿‡")

            # é˜¶æ®µ7: æ¨¡å‹å¯¹æ¯”ï¼ˆå¯é€‰ï¼‰
            if self.args.compare_models:
                self._run_stage("æ¨¡å‹å¯¹æ¯”", self.compare_models_perf)
            else:
                print("\n[é˜¶æ®µ7/8] æ¨¡å‹å¯¹æ¯”")
                print("  âŠ˜ å·²è·³è¿‡")

            # é˜¶æ®µ8: ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
            self._run_stage("ç”Ÿæˆæ€»ç»“æŠ¥å‘Š", self.generate_summary)

            self.results['end_time'] = time.time()
            self.print_final_summary()

        except Exception as e:
            print(f"\nâŒ æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            self.results['errors'].append(str(e))
            sys.exit(1)

    def _run_stage(self, name, func):
        """è¿è¡Œå•ä¸ªé˜¶æ®µå¹¶è®°å½•æ—¶é—´"""
        stage_num = len([s for s in self.results['stages'] if self.results['stages'][s].get('completed', False)]) + 1
        print(f"\n[é˜¶æ®µ{stage_num}/8] {name}")

        start = time.time()
        try:
            func()
            elapsed = time.time() - start
            self.results['stages'][name] = {
                'completed': True,
                'time': elapsed,
                'error': None
            }
        except Exception as e:
            elapsed = time.time() - start
            self.results['stages'][name] = {
                'completed': False,
                'time': elapsed,
                'error': str(e)
            }
            raise

    def setup(self):
        """é˜¶æ®µ1: åˆå§‹åŒ–"""
        # åˆ›å»ºè¿è¡Œç›®å½•
        if self.args.run_name:
            run_name = self.args.run_name
        else:
            run_name = datetime.now().strftime("%Y%m%d_%H%M%S")

        self.run_dir = Path(self.args.output_base) / run_name
        self.run_dir.mkdir(parents=True, exist_ok=True)

        print(f"  âœ“ åˆ›å»ºè¿è¡Œç›®å½•: {self.run_dir}")

        # åˆ›å»ºå­ç›®å½•
        (self.run_dir / 'checkpoints').mkdir(exist_ok=True)
        (self.run_dir / 'metrics').mkdir(exist_ok=True)
        (self.run_dir / 'visualizations').mkdir(exist_ok=True)
        (self.run_dir / 'exported_models').mkdir(exist_ok=True)
        (self.run_dir / 'test_results').mkdir(exist_ok=True)

        # ä¿å­˜é…ç½®
        config_dict = vars(self.args).copy()
        config_dict['run_dir'] = str(self.run_dir)
        config_dict['timestamp'] = datetime.now().isoformat()

        with open(self.run_dir / 'config.json', 'w', encoding='utf-8') as f:
            json.dump(config_dict, f, indent=2, ensure_ascii=False)

        print("  âœ“ ä¿å­˜è¿è¡Œé…ç½®")
        print(f"  è¿è¡Œç›®å½•: {self.run_dir}")

    def prepare_data(self):
        """é˜¶æ®µ2: æ•°æ®å‡†å¤‡"""
        processed_dir = Path('data/processed')

        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°åˆ’åˆ†
        if not processed_dir.exists() or self.args.force_split:
            print("  æ­£åœ¨åˆ’åˆ†æ•°æ®é›†...")
            from src.config.training_config import BaseConfig
            from src.data.split_data import split_dataset

            base_config = BaseConfig()
            split_dataset(base_config)
            print("  âœ“ æ•°æ®é›†åˆ’åˆ†å®Œæˆ")
        else:
            print("  âœ“ æ•°æ®é›†å·²å­˜åœ¨")

        # ç»Ÿè®¡æ•°æ®
        train_count = len(list((processed_dir / 'train').rglob('*.jpg'))) + \
                     len(list((processed_dir / 'train').rglob('*.png')))
        val_count = len(list((processed_dir / 'val').rglob('*.jpg'))) + \
                   len(list((processed_dir / 'val').rglob('*.png')))
        test_count = len(list((processed_dir / 'test').rglob('*.jpg'))) + \
                    len(list((processed_dir / 'test').rglob('*.png')))

        print(f"  âœ“ è®­ç»ƒé›†: {train_count}å¼  | éªŒè¯é›†: {val_count}å¼  | æµ‹è¯•é›†: {test_count}å¼ ")

        self.results['data'] = {
            'train': train_count,
            'val': val_count,
            'test': test_count
        }

    def train_model(self):
        """é˜¶æ®µ3: æ¨¡å‹è®­ç»ƒ"""
        from src.config.training_config import TrainingConfig
        from src.data.dataloader import create_dataloaders
        from src.models.model_factory import create_model
        from src.training.trainer import Trainer

        # åˆ›å»ºé…ç½®
        config = TrainingConfig(
            num_epochs=self.args.epochs,
            batch_size=self.args.batch_size,
            learning_rate=self.args.lr,
            data_root='data/processed'
        )

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader, val_loader, test_loader = create_dataloaders(
            data_root='data/processed',
            batch_size=self.args.batch_size,
            num_workers=4,
            img_size=self.args.img_size
        )

        # åˆ›å»ºæ¨¡å‹
        model = create_model(
            model_name=self.args.model,
            num_classes=len(config.class_names),
            pretrained=not self.args.no_pretrained,
            dropout=self.args.dropout
        )

        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = Trainer(
            model=model,
            train_loader=train_loader,
            val_loader=val_loader,
            config=config
        )

        # æ‰§è¡Œè®­ç»ƒ
        if self.args.two_stage:
            print("  è®­ç»ƒæ¨¡å¼: ä¸¤é˜¶æ®µè®­ç»ƒ")
            print(f"    é˜¶æ®µ1: å†»ç»“ä¸»å¹² ({self.args.stage1_epochs} epochs)")
            print(f"    é˜¶æ®µ2: å¾®è°ƒå…¨æ¨¡å‹ ({self.args.stage2_epochs} epochs)")

            # å¯¼å…¥ä¸¤é˜¶æ®µè®­ç»ƒå‡½æ•°
            import argparse as ap

            from scripts.train import train_two_stage

            # åˆ›å»ºä¸´æ—¶å‚æ•°
            train_args = ap.Namespace(
                data_root='data/processed',
                model=self.args.model,
                pretrained=not self.args.no_pretrained,
                batch_size=self.args.batch_size,
                lr=self.args.lr,
                stage1_epochs=self.args.stage1_epochs,
                stage2_epochs=self.args.stage2_epochs,
                stage2_lr=self.args.stage2_lr,
                unfreeze_from=self.args.unfreeze_from,
                num_workers=self.args.num_workers,
                device='auto',
                patience=self.args.patience,
                dropout=self.args.dropout,
                img_size=224,
                weight_decay=1e-4
            )

            _, history = train_two_stage(train_args)
        else:
            print(f"  è®­ç»ƒæ¨¡å¼: æ ‡å‡†è®­ç»ƒ ({self.args.epochs} epochs)")
            history = trainer.train()

        # ä¿å­˜checkpointåˆ°è¿è¡Œç›®å½•
        checkpoint_src = Path('data/output/checkpoints/best_model.pth')
        self.checkpoint_path = self.run_dir / 'checkpoints' / 'best_model.pth'

        if checkpoint_src.exists():
            shutil.copy(checkpoint_src, self.checkpoint_path)
            print("  âœ“ æ¨¡å‹å·²ä¿å­˜: checkpoints/best_model.pth")

        # è®°å½•è®­ç»ƒç»“æœ
        if history:
            best_val_acc = max(history['val_acc']) if 'val_acc' in history else 0
            self.results['training'] = {
                'best_val_acc': best_val_acc,
                'epochs': len(history['val_acc']) if 'val_acc' in history else 0,
                'history': history
            }
            print(f"  âœ“ æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc*100:.2f}%")

    def load_existing_model(self):
        """åŠ è½½å·²æœ‰æ¨¡å‹"""
        if self.args.checkpoint:
            src_checkpoint = Path(self.args.checkpoint)
        else:
            src_checkpoint = Path('data/output/checkpoints/best_model.pth')

        if not src_checkpoint.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶: {src_checkpoint}")

        # å¤åˆ¶åˆ°è¿è¡Œç›®å½•
        self.checkpoint_path = self.run_dir / 'checkpoints' / 'best_model.pth'
        shutil.copy(src_checkpoint, self.checkpoint_path)

        print(f"  ä½¿ç”¨æ¨¡å‹: {src_checkpoint}")

    def evaluate_model(self):
        """é˜¶æ®µ4: æ¨¡å‹è¯„ä¼°"""
        from src.data.dataloader import create_dataloaders
        from src.models.model_factory import load_model_from_checkpoint
        from src.training.metrics import MetricsCalculator
        from src.utils.device import get_device
        from src.utils.visualization import Visualizer

        # åŠ è½½æ¨¡å‹
        model, checkpoint = load_model_from_checkpoint(str(self.checkpoint_path))
        device = get_device()

        # åŠ è½½æµ‹è¯•é›†
        _, _, test_loader = create_dataloaders(
            'data/processed',
            batch_size=self.args.batch_size,
            num_workers=4,
            img_size=self.args.img_size
        )

        # è¯„ä¼°
        model.to(device)
        model.eval()

        y_true = []
        y_pred = []

        with torch.no_grad():
            for images, labels in test_loader:
                images = images.to(device)
                outputs = model(images)
                _, predicted = torch.max(outputs, 1)

                y_true.extend(labels.cpu().numpy())
                y_pred.extend(predicted.cpu().numpy())

        # è®¡ç®—æŒ‡æ ‡
        class_names = checkpoint.get('config').class_names if 'config' in checkpoint else ['Failure', 'Loading', 'Success']
        metrics_calculator = MetricsCalculator(class_names)
        metrics = metrics_calculator.compute_metrics(y_true, y_pred)

        # ä¿å­˜ç»“æœ
        metrics_dir = self.run_dir / 'metrics'
        vis_dir = self.run_dir / 'visualizations'
        metrics_dir.mkdir(parents=True, exist_ok=True)
        vis_dir.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜æŒ‡æ ‡JSON
        metrics_json = {k: v for k, v in metrics.items() if k != 'confusion_matrix_array'}
        with open(metrics_dir / 'test_metrics.json', 'w', encoding='utf-8') as f:
            json.dump(metrics_json, f, indent=2, ensure_ascii=False)

        # ä¿å­˜åˆ†ç±»æŠ¥å‘Š
        with open(metrics_dir / 'classification_report.txt', 'w', encoding='utf-8') as f:
            report = metrics_calculator.get_classification_report(y_true, y_pred)
            f.write(report)

        # ç”Ÿæˆå¯è§†åŒ–
        Visualizer.plot_confusion_matrix(
            metrics['confusion_matrix_array'],
            class_names,
            save_path=str(vis_dir / 'confusion_matrix.png')
        )

        Visualizer.plot_per_class_metrics(
            metrics,
            save_path=str(vis_dir / 'per_class_metrics.png')
        )

        # ç»˜åˆ¶è®­ç»ƒå†å²ï¼ˆå¦‚æœæœ‰ï¼‰
        if 'history' in checkpoint:
            Visualizer.plot_training_history(
                checkpoint['history'],
                save_path=str(vis_dir / 'training_history.png')
            )

        accuracy = metrics.get('accuracy', 0)
        print(f"  âœ“ æµ‹è¯•é›†å‡†ç¡®ç‡: {accuracy*100:.2f}%")
        print("  âœ“ æŒ‡æ ‡å·²ä¿å­˜: metrics/")
        print("  âœ“ å¯è§†åŒ–å·²ç”Ÿæˆ: visualizations/")

        self.results['evaluation'] = metrics

    def export_models(self):
        """é˜¶æ®µ5: æ¨¡å‹å¯¼å‡º"""
        export_dir = self.run_dir / 'exported_models'
        export_dir.mkdir(exist_ok=True)

        formats = self.args.export_formats.split()

        for fmt in formats:
            try:
                if fmt == 'onnx':
                    from src.export.onnx_exporter import ONNXExporter
                    from src.models.model_factory import \
                        load_model_from_checkpoint

                    model, _ = load_model_from_checkpoint(str(self.checkpoint_path))
                    exporter = ONNXExporter(model, img_size=224)
                    output_path = export_dir / 'model.onnx'
                    exporter.export(str(output_path))

                    print("  âœ“ ONNXå¯¼å‡ºæˆåŠŸ")

                elif fmt == 'coreml':
                    from src.export.coreml_exporter import CoreMLExporter
                    from src.models.model_factory import \
                        load_model_from_checkpoint

                    model, checkpoint = load_model_from_checkpoint(str(self.checkpoint_path))
                    class_names = checkpoint.get('config').class_names if 'config' in checkpoint else ['Failure', 'Loading', 'Success']

                    exporter = CoreMLExporter(model, img_size=224, class_names=class_names)
                    output_path = export_dir / 'model.mlpackage'
                    exporter.export(str(output_path), quantize=self.args.quantize)

                    print("  âœ“ CoreMLå¯¼å‡ºæˆåŠŸ")

                elif fmt == 'tflite':
                    print("  âš  TFLiteå¯¼å‡ºéœ€è¦Python 3.11ç¯å¢ƒï¼Œå·²è·³è¿‡")

            except Exception as e:
                print(f"  âœ— {fmt.upper()}å¯¼å‡ºå¤±è´¥: {e}")

    def test_inference(self):
        """é˜¶æ®µ6: æµ‹è¯•å›¾ç‰‡æ¨ç†"""
        test_images_dir = Path(self.args.test_images)

        if not test_images_dir.exists():
            print(f"  âš  æµ‹è¯•å›¾ç‰‡ç›®å½•ä¸å­˜åœ¨: {test_images_dir}")
            return

        # æ‰§è¡Œæ‰¹é‡æ¨ç†
        import argparse as ap

        test_results_dir = self.run_dir / 'test_results'
        test_results_dir.mkdir(exist_ok=True)

        # åˆ›å»ºæ¨ç†å‚æ•°
        infer_args = ap.Namespace(
            checkpoint=str(self.checkpoint_path),
            input_dir=str(test_images_dir),
            output=str(test_results_dir / 'predictions.json'),
            img_size=224,
            batch_size=1,
            copy_to_folders=self.args.copy_images,
            output_dir=str(self.run_dir / 'classified_images'),
            output_csv=str(test_results_dir / 'predictions.csv'),
            measure_time=True
        )

        # ä¸´æ—¶ä¿®æ”¹sys.argvæ¥è°ƒç”¨batch_inference
        original_argv = sys.argv
        sys.argv = ['batch_inference.py'] + [
            '--checkpoint', str(infer_args.checkpoint),
            '--input-dir', str(infer_args.input_dir),
            '--output', str(infer_args.output),
            '--measure-time'
        ]

        if infer_args.copy_to_folders:
            sys.argv.extend(['--copy-to-folders', '--output-dir', str(infer_args.output_dir)])

        try:
            # ç›´æ¥è°ƒç”¨æ¨ç†é€»è¾‘ï¼ˆé¿å…é‡å¤è§£æå‚æ•°ï¼‰
            import csv

            from PIL import Image
            from tqdm import tqdm

            from src.data.transforms import get_val_transforms
            from src.models.model_factory import load_model_from_checkpoint
            from src.utils.device import get_device

            device = get_device()
            model, checkpoint = load_model_from_checkpoint(str(self.checkpoint_path))
            model.to(device)
            model.eval()

            class_names = checkpoint.get('config').class_names if 'config' in checkpoint else ['Failure', 'Loading', 'Success']
            transform = get_val_transforms(img_size=224)

            # è·å–å›¾ç‰‡
            image_extensions = {'.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG'}
            image_paths = [p for p in test_images_dir.iterdir() if p.is_file() and p.suffix in image_extensions]

            results = {}
            class_counts = {name: 0 for name in class_names}

            for image_path in tqdm(image_paths, desc="  å¤„ç†å›¾ç‰‡"):
                try:
                    image = Image.open(image_path).convert('RGB')
                    image_tensor = transform(image)

                    # æ¨ç†
                    start_time = time.perf_counter()
                    with torch.no_grad():
                        image_tensor = image_tensor.unsqueeze(0).to(device)
                        outputs = model(image_tensor)
                        probabilities = torch.nn.functional.softmax(outputs, dim=1)
                        confidence, predicted = torch.max(probabilities, 1)
                    end_time = time.perf_counter()

                    pred_class = class_names[predicted.item()]
                    conf_score = confidence.item()
                    inference_time_ms = (end_time - start_time) * 1000

                    results[image_path.name] = {
                        'predicted_class': pred_class,
                        'confidence': conf_score,
                        'inference_time_ms': inference_time_ms,
                        'probabilities': {
                            class_names[i]: float(probabilities[0][i])
                            for i in range(len(class_names))
                        }
                    }

                    class_counts[pred_class] += 1
                except Exception as e:
                    print(f"    âš  {image_path.name} å¤„ç†å¤±è´¥: {e}")

            # ä¿å­˜JSON
            with open(test_results_dir / 'predictions.json', 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)

            # ä¿å­˜CSV
            with open(test_results_dir / 'predictions.csv', 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=['filename', 'predicted_class', 'confidence', 'inference_time_ms'])
                writer.writeheader()
                for filename, pred in results.items():
                    writer.writerow({
                        'filename': filename,
                        'predicted_class': pred['predicted_class'],
                        'confidence': f"{pred['confidence']:.4f}",
                        'inference_time_ms': f"{pred['inference_time_ms']:.2f}"
                    })

            # ä¿å­˜ç»Ÿè®¡æ‘˜è¦
            total = sum(class_counts.values())
            with open(test_results_dir / 'summary.txt', 'w', encoding='utf-8') as f:
                f.write("æµ‹è¯•å›¾ç‰‡åˆ†ç±»ç»Ÿè®¡\n")
                f.write("="*50 + "\n")
                for class_name in class_names:
                    count = class_counts[class_name]
                    percentage = (count / total * 100) if total > 0 else 0
                    f.write(f"{class_name}: {count}å¼  ({percentage:.1f}%)\n")
                f.write(f"æ€»è®¡: {total}å¼ \n")

            # æ‰“å°ç»Ÿè®¡
            for class_name in class_names:
                count = class_counts[class_name]
                percentage = (count / total * 100) if total > 0 else 0
                print(f"  âœ“ {class_name}: {count}å¼  ({percentage:.1f}%)")

            print("  âœ“ ç»“æœå·²ä¿å­˜: test_results/")

            self.results['test_inference'] = {
                'total': total,
                'class_counts': class_counts
            }

        finally:
            sys.argv = original_argv

    def compare_models_perf(self):
        """é˜¶æ®µ7: æ¨¡å‹å¯¹æ¯”"""
        # æ£€æŸ¥æ˜¯å¦æœ‰å¯¼å‡ºçš„æ¨¡å‹
        export_dir = self.run_dir / 'exported_models'

        onnx_path = export_dir / 'model.onnx'
        coreml_path = export_dir / 'model.mlpackage'

        if not onnx_path.exists():
            print("  âš  æœªæ‰¾åˆ°ONNXæ¨¡å‹ï¼Œè·³è¿‡å¯¹æ¯”")
            return

        # æ‰§è¡Œå¯¹æ¯”
        from scripts.compare_models import ModelComparator

        models_config = {
            'pytorch': {'path': str(self.checkpoint_path), 'type': 'checkpoint'},
            'onnx': {'path': str(onnx_path), 'type': 'onnx'}
        }

        class_names = ['Failure', 'Loading', 'Success']
        comparator = ModelComparator(models_config, 'data/processed/test', class_names)

        results = comparator.compare(num_samples=None)

        if results:
            # ä¿å­˜ç»“æœ
            comparison_dir = self.run_dir / 'model_comparison'
            comparison_dir.mkdir(exist_ok=True)

            with open(comparison_dir / 'comparison.json', 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)

            # ç”ŸæˆMarkdownæŠ¥å‘Š
            from scripts.compare_models import generate_markdown_report
            generate_markdown_report(results, comparison_dir / 'comparison.md')

            print("  âœ“ å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜: model_comparison/")

            self.results['model_comparison'] = results['summary']

    def generate_summary(self):
        """é˜¶æ®µ8: ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
        total_time = time.time() - self.results['start_time']

        # ç”ŸæˆMarkdownæŠ¥å‘Š
        report = []
        report.append("# æ¨¡å‹è®­ç»ƒæµæ°´çº¿è¿è¡ŒæŠ¥å‘Š\n")
        report.append(f"è¿è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"è¿è¡Œåç§°: {self.run_dir.name}")
        report.append(f"æ€»è€—æ—¶: {format_time(total_time)}\n")
        report.append("---\n")

        # 1. è¿è¡Œé…ç½®
        report.append("## 1. è¿è¡Œé…ç½®\n")
        report.append(f"- è®­ç»ƒæ¨¡å¼: {'ä¸¤é˜¶æ®µè®­ç»ƒ' if self.args.two_stage else 'æ ‡å‡†è®­ç»ƒ'}")
        if self.args.two_stage:
            report.append(f"- é˜¶æ®µ1: {self.args.stage1_epochs} epochs (å†»ç»“ä¸»å¹²)")
            report.append(f"- é˜¶æ®µ2: {self.args.stage2_epochs} epochs (å¾®è°ƒå…¨æ¨¡å‹)")
        else:
            report.append(f"- Epochs: {self.args.epochs}")
        report.append(f"- æ‰¹æ¬¡å¤§å°: {self.args.batch_size}")
        report.append(f"- å­¦ä¹ ç‡: {self.args.lr}")
        report.append(f"- å¯¼å‡ºæ ¼å¼: {self.args.export_formats}\n")
        report.append("---\n")

        # 2. è®­ç»ƒç»“æœ
        if 'training' in self.results:
            training = self.results['training']
            report.append("## 2. è®­ç»ƒç»“æœ\n")
            report.append(f"- æœ€ä½³éªŒè¯å‡†ç¡®ç‡: **{training.get('best_val_acc', 0)*100:.2f}%**")
            report.append(f"- è®­ç»ƒè½®æ•°: {training.get('epochs', 0)}")
            if 'æ¨¡å‹è®­ç»ƒ' in self.results['stages']:
                train_time = self.results['stages']['æ¨¡å‹è®­ç»ƒ']['time']
                report.append(f"- è®­ç»ƒè€—æ—¶: {format_time(train_time)}\n")
            report.append("---\n")

        # 3. è¯„ä¼°æŒ‡æ ‡
        if 'evaluation' in self.results:
            eval_metrics = self.results['evaluation']
            report.append("## 3. è¯„ä¼°æŒ‡æ ‡\n")
            report.append(f"- æµ‹è¯•é›†å‡†ç¡®ç‡: **{eval_metrics.get('accuracy', 0)*100:.2f}%**")

            if 'per_class' in eval_metrics:
                report.append("\n### å„ç±»åˆ«è¡¨ç°\n")
                report.append("| ç±»åˆ« | Precision | Recall | F1-Score |")
                report.append("|------|-----------|--------|----------|")
                for class_name, metrics in eval_metrics['per_class'].items():
                    report.append(f"| {class_name} | {metrics['precision']*100:.2f}% | {metrics['recall']*100:.2f}% | {metrics['f1_score']*100:.2f}% |")
            report.append("\n---\n")

        # 4. æµ‹è¯•å›¾ç‰‡åˆ†ç±»
        if 'test_inference' in self.results:
            test_inf = self.results['test_inference']
            report.append("## 4. æµ‹è¯•å›¾ç‰‡åˆ†ç±»\n")
            report.append(f"- æ€»å›¾ç‰‡æ•°: {test_inf['total']}å¼ ")
            for class_name, count in test_inf['class_counts'].items():
                percentage = (count / test_inf['total'] * 100) if test_inf['total'] > 0 else 0
                report.append(f"- {class_name}: {count}å¼  ({percentage:.1f}%)")
            report.append("\n---\n")

        # 5. æ¨¡å‹å¯¹æ¯”
        if 'model_comparison' in self.results:
            report.append("## 5. æ¨¡å‹å¯¹æ¯”\n")
            comp = self.results['model_comparison']

            report.append("| æ¨¡å‹ | å¹³å‡è€—æ—¶ (ms) |")
            report.append("|------|---------------|")
            for model_name, stats in comp.items():
                if model_name == 'consistency':
                    continue
                report.append(f"| {model_name} | {stats.get('avg_time_ms', 'N/A')} |")
            report.append("\n---\n")

        # 6. è¾“å‡ºæ–‡ä»¶æ¸…å•
        report.append("## 6. è¾“å‡ºæ–‡ä»¶æ¸…å•\n")
        report.append("```")
        report.append(f"{self.run_dir.name}/")

        for subdir in ['checkpoints', 'metrics', 'visualizations', 'exported_models', 'test_results']:
            path = self.run_dir / subdir
            if path.exists():
                files = list(path.iterdir())
                if files:
                    report.append(f"â”œâ”€â”€ {subdir}/")
                    for file in files[:5]:  # æœ€å¤šæ˜¾ç¤º5ä¸ªæ–‡ä»¶
                        report.append(f"â”‚   â”œâ”€â”€ {file.name}")

        report.append("â””â”€â”€ run_summary.md")
        report.append("```\n")

        report.append("---\n")
        report.append("*æŠ¥å‘Šç”± pipeline.py è‡ªåŠ¨ç”Ÿæˆ*")

        # ä¿å­˜æŠ¥å‘Š
        with open(self.run_dir / 'run_summary.md', 'w', encoding='utf-8') as f:
            f.write('\n'.join(report))

        print("  âœ“ æ€»ç»“å·²ä¿å­˜: run_summary.md")

    def print_final_summary(self):
        """æ‰“å°æœ€ç»ˆæ€»ç»“"""
        total_time = self.results['end_time'] - self.results['start_time']

        print("\n" + "="*70)
        print("âœ… æµæ°´çº¿æ‰§è¡Œå®Œæˆï¼")
        print("="*70)
        print(f"æ€»è€—æ—¶: {format_time(total_time)}")
        print(f"è¾“å‡ºç›®å½•: {self.run_dir}")
        print()
        print("æŸ¥çœ‹å®Œæ•´æŠ¥å‘Š:")
        print(f"  cat {self.run_dir}/run_summary.md")
        print()
        if (self.run_dir / 'metrics' / 'test_metrics.json').exists():
            print("æŸ¥çœ‹è¯„ä¼°æŒ‡æ ‡:")
            print(f"  cat {self.run_dir}/metrics/test_metrics.json")
        print("="*70 + "\n")


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='ä¸€é”®è®­ç»ƒæµæ°´çº¿')

    # è®­ç»ƒç›¸å…³
    parser.add_argument('--epochs', type=int, default=30,
                        help='è®­ç»ƒepochæ•°')
    parser.add_argument('--batch-size', type=int, default=16,
                        help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--lr', type=float, default=1e-3,
                        help='å­¦ä¹ ç‡')
    parser.add_argument('--two-stage', action='store_true',
                        help='å¯ç”¨ä¸¤é˜¶æ®µè®­ç»ƒ')
    parser.add_argument('--stage1-epochs', type=int, default=10,
                        help='é˜¶æ®µ1 epochæ•°')
    parser.add_argument('--stage2-epochs', type=int, default=20,
                        help='é˜¶æ®µ2 epochæ•°')
    parser.add_argument('--stage2-lr', type=float, default=1e-4,
                        help='é˜¶æ®µ2å­¦ä¹ ç‡')
    parser.add_argument('--unfreeze-from', type=int, default=14,
                        help='ä»ç¬¬å‡ å±‚å¼€å§‹è§£å†»')
    parser.add_argument('--model', type=str, default='mobilenet_v2',
                        help='æ¨¡å‹æ¶æ„')
    parser.add_argument('--no-pretrained', action='store_true',
                        help='ä¸ä½¿ç”¨é¢„è®­ç»ƒæƒé‡')
    parser.add_argument('--dropout', type=float, default=0.3,
                        help='Dropoutæ¯”ä¾‹')
    parser.add_argument('--patience', type=int, default=10,
                        help='æ—©åœè€å¿ƒå€¼')
    parser.add_argument('--num-workers', type=int, default=4,
                        help='æ•°æ®åŠ è½½çº¿ç¨‹æ•°')

    # æ•°æ®ç›¸å…³
    parser.add_argument('--train-data', type=str, default='data/input/data1226',
                        help='è®­ç»ƒæ•°æ®è·¯å¾„')
    parser.add_argument('--test-images', type=str, default='data/test_images',
                        help='æµ‹è¯•å›¾ç‰‡è·¯å¾„')
    parser.add_argument('--img-size', type=int, default=224,
                        help='è¾“å…¥å›¾åƒå°ºå¯¸')
    parser.add_argument('--force-split', action='store_true',
                        help='å¼ºåˆ¶é‡æ–°åˆ’åˆ†æ•°æ®é›†')

    # å¯¼å‡ºç›¸å…³
    parser.add_argument('--export-formats', type=str, default='onnx coreml',
                        help='å¯¼å‡ºæ ¼å¼ï¼ˆç©ºæ ¼åˆ†éš”ï¼‰')
    parser.add_argument('--quantize', action='store_true',
                        help='é‡åŒ–æ¨¡å‹')

    # è¾“å‡ºç›¸å…³
    parser.add_argument('--output-base', type=str, default='data/output/runs',
                        help='è¾“å‡ºåŸºç¡€ç›®å½•')
    parser.add_argument('--run-name', type=str, default=None,
                        help='è‡ªå®šä¹‰è¿è¡Œåç§°')
    parser.add_argument('--copy-images', action='store_true',
                        help='å¤åˆ¶åˆ†ç±»åçš„å›¾ç‰‡')

    # æµç¨‹æ§åˆ¶
    parser.add_argument('--skip-train', action='store_true',
                        help='è·³è¿‡è®­ç»ƒ')
    parser.add_argument('--checkpoint', type=str, default=None,
                        help='ä½¿ç”¨æŒ‡å®šcheckpoint')
    parser.add_argument('--skip-export', action='store_true',
                        help='è·³è¿‡æ¨¡å‹å¯¼å‡º')
    parser.add_argument('--skip-test', action='store_true',
                        help='è·³è¿‡æµ‹è¯•å›¾ç‰‡æ¨ç†')
    parser.add_argument('--compare-models', action='store_true',
                        help='å¯¹æ¯”ä¸åŒæ ¼å¼æ¨¡å‹æ€§èƒ½')

    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()

    runner = PipelineRunner(args)
    runner.run()


if __name__ == '__main__':
    main()
