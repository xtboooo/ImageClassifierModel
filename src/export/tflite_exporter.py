"""TensorFlow Lite æ¨¡å‹å¯¼å‡ºå™¨ï¼ˆAndroidï¼‰"""
import torch
import numpy as np
from pathlib import Path


class TFLiteExporter:
    """TFLite æ¨¡å‹å¯¼å‡ºå™¨ï¼ˆç”¨äº Android éƒ¨ç½²ï¼‰"""

    def __init__(self, model, img_size=224, class_names=None):
        """
        Args:
            model: PyTorch æ¨¡å‹
            img_size: è¾“å…¥å›¾åƒå°ºå¯¸
            class_names: ç±»åˆ«åç§°åˆ—è¡¨
        """
        self.model = model
        self.img_size = img_size
        self.class_names = class_names or ['Failure', 'Loading', 'Success']

    def export(self, save_path, quantize=False):
        """
        å¯¼å‡ºä¸º TFLite æ ¼å¼ï¼ˆé€šè¿‡ ONNX ä¸­é—´æ ¼å¼ï¼‰

        Args:
            save_path: ä¿å­˜è·¯å¾„
            quantize: æ˜¯å¦è¿›è¡Œé‡åŒ–ï¼ˆå‡å°æ¨¡å‹å¤§å°ï¼‰

        Returns:
            str: ä¿å­˜è·¯å¾„
        """
        print(f"\nå¯¼å‡º TFLite æ¨¡å‹åˆ°: {save_path}")

        try:
            # æ£€æŸ¥ä¾èµ–
            try:
                import onnx
                from onnx_tf.backend import prepare
                import tensorflow as tf
            except ImportError as e:
                print("âŒ ç¼ºå°‘å¿…è¦çš„ä¾èµ–åŒ…")
                print("\nè¯·å®‰è£…ä»¥ä¸‹ä¾èµ–:")
                print("  pip install onnx onnx-tf tensorflow")
                print("\næˆ–ä½¿ç”¨ uv:")
                print("  uv pip install onnx onnx-tf tensorflow")
                raise ImportError(f"Missing dependencies: {e}")

            # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            self.model.eval()
            self.model.cpu()

            # æ­¥éª¤ 1: å…ˆå¯¼å‡ºä¸º ONNX
            print("æ­¥éª¤ 1/3: å¯¼å‡ºä¸º ONNX ä¸­é—´æ ¼å¼...")
            onnx_path = str(Path(save_path).with_suffix('.onnx'))
            dummy_input = torch.randn(1, 3, self.img_size, self.img_size)

            torch.onnx.export(
                self.model,
                dummy_input,
                onnx_path,
                opset_version=14,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}},
                export_params=True,
                do_constant_folding=True
            )
            print("  âœ“ ONNX å¯¼å‡ºå®Œæˆ")

            # æ­¥éª¤ 2: è½¬æ¢ ONNX åˆ° TensorFlow
            print("æ­¥éª¤ 2/3: è½¬æ¢ ONNX åˆ° TensorFlow...")
            onnx_model = onnx.load(onnx_path)
            tf_rep = prepare(onnx_model)

            # å¯¼å‡ºä¸º TensorFlow SavedModel
            tf_model_dir = str(Path(save_path).with_suffix('.tf'))
            tf_rep.export_graph(tf_model_dir)
            print(f"  âœ“ TensorFlow æ¨¡å‹å·²ä¿å­˜åˆ°: {tf_model_dir}")

            # æ­¥éª¤ 3: è½¬æ¢ä¸º TFLite
            print("æ­¥éª¤ 3/3: è½¬æ¢ä¸º TFLite...")
            converter = tf.lite.TFLiteConverter.from_saved_model(tf_model_dir)

            if quantize:
                print("  å¯ç”¨é‡åŒ–...")
                converter.optimizations = [tf.lite.Optimize.DEFAULT]
                converter.target_spec.supported_types = [tf.float16]

            tflite_model = converter.convert()

            # ä¿å­˜ TFLite æ¨¡å‹
            with open(save_path, 'wb') as f:
                f.write(tflite_model)

            print(f"  âœ“ TFLite æ¨¡å‹å·²ä¿å­˜")

            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            print("æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
            import shutil
            if Path(onnx_path).exists():
                Path(onnx_path).unlink()
            if Path(tf_model_dir).exists():
                shutil.rmtree(tf_model_dir)

            # æ‰“å°æ¨¡å‹ä¿¡æ¯
            self._print_model_info(save_path, quantize)

            # æµ‹è¯•æ¨¡å‹
            self._test_inference(save_path, dummy_input)

            print(f"âœ“ TFLite æ¨¡å‹å¯¼å‡ºæˆåŠŸ: {save_path}\n")
            return save_path

        except Exception as e:
            print(f"âŒ TFLite å¯¼å‡ºå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            print("\nğŸ’¡ æç¤º: TFLite å¯¼å‡ºä¾èµ–è¾ƒå¤š,å¦‚é‡é—®é¢˜å¯ä»¥:")
            print("  1. ä¼˜å…ˆä½¿ç”¨ ONNX æ ¼å¼è¿›è¡Œè·¨å¹³å°éƒ¨ç½²")
            print("  2. ä½¿ç”¨åœ¨çº¿å·¥å…·è½¬æ¢ ONNX -> TFLite")
            print("  3. æ‰‹åŠ¨ä½¿ç”¨ onnx-tf å’Œ tensorflow å·¥å…·é“¾è½¬æ¢")
            raise

    def _test_inference(self, tflite_path, test_input):
        """
        æµ‹è¯• TFLite æ¨¡å‹æ¨ç†

        Args:
            tflite_path: TFLite æ¨¡å‹è·¯å¾„
            test_input: æµ‹è¯•è¾“å…¥
        """
        try:
            import tensorflow as tf

            print("æµ‹è¯• TFLite æ¨¡å‹æ¨ç†...")

            # åŠ è½½ TFLite æ¨¡å‹
            interpreter = tf.lite.Interpreter(model_path=tflite_path)
            interpreter.allocate_tensors()

            # è·å–è¾“å…¥è¾“å‡ºè¯¦æƒ…
            input_details = interpreter.get_input_details()
            output_details = interpreter.get_output_details()

            # å‡†å¤‡è¾“å…¥æ•°æ®
            input_data = test_input.numpy().astype(np.float32)
            interpreter.set_tensor(input_details[0]['index'], input_data)

            # è¿è¡Œæ¨ç†
            interpreter.invoke()

            # è·å–è¾“å‡º
            output_data = interpreter.get_tensor(output_details[0]['index'])

            # PyTorch æ¨ç†å¯¹æ¯”
            with torch.no_grad():
                pytorch_output = self.model(test_input).numpy()

            # éªŒè¯ä¸€è‡´æ€§
            try:
                np.testing.assert_allclose(
                    pytorch_output, output_data,
                    rtol=1e-2, atol=1e-3  # TFLite è½¬æ¢å¯èƒ½æœ‰è¾ƒå¤§è¯¯å·®
                )
                print("âœ“ æ¨ç†ä¸€è‡´æ€§éªŒè¯é€šè¿‡")
            except AssertionError:
                max_diff = np.max(np.abs(pytorch_output - output_data))
                print(f"âš ï¸  æ¨ç†ç»“æœå­˜åœ¨å·®å¼‚(æœ€å¤§å·®å¼‚: {max_diff:.6f})")
                print("   è¿™æ˜¯æ­£å¸¸çš„,TFLite è½¬æ¢å¯èƒ½å¼•å…¥å°‘é‡æ•°å€¼å·®å¼‚")

        except Exception as e:
            print(f"âš ï¸  æ¨ç†æµ‹è¯•å¤±è´¥: {e}")

    def _print_model_info(self, model_path, quantize):
        """
        æ‰“å° TFLite æ¨¡å‹ä¿¡æ¯

        Args:
            model_path: æ¨¡å‹è·¯å¾„
            quantize: æ˜¯å¦é‡åŒ–
        """
        import os

        # æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(model_path) / (1024 * 1024)  # MB

        print("\nTFLite æ¨¡å‹ä¿¡æ¯:")
        print(f"  æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
        print(f"  é‡åŒ–: {'æ˜¯ (FP16)' if quantize else 'å¦ (FP32)'}")
        print(f"  ç±»åˆ«: {', '.join(self.class_names)}")
        print(f"  è¾“å…¥å°ºå¯¸: (1, {self.img_size}, {self.img_size}, 3)")
        print("  æ³¨æ„: TFLite ä½¿ç”¨ NHWC æ ¼å¼ (PyTorch ä½¿ç”¨ NCHW)")
