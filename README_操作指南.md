# ImageClassifierModel å®Œæ•´æ“ä½œæŒ‡å—

æœ¬æŒ‡å—æä¾›ä»é›¶å¼€å§‹è®­ç»ƒã€å¯¼å‡ºå’Œä½¿ç”¨å›¾åƒåˆ†ç±»æ¨¡å‹çš„è¯¦ç»†æ­¥éª¤ã€‚

---

## ğŸ“‹ ç›®å½•

1. [ç¯å¢ƒå‡†å¤‡](#1-ç¯å¢ƒå‡†å¤‡)
2. [æ•°æ®å‡†å¤‡](#2-æ•°æ®å‡†å¤‡)
3. [æ¨¡å‹è®­ç»ƒ](#3-æ¨¡å‹è®­ç»ƒ)
4. [æ¨¡å‹è¯„ä¼°](#4-æ¨¡å‹è¯„ä¼°)
5. [æ¨¡å‹å¯¼å‡º](#5-æ¨¡å‹å¯¼å‡º)
6. [æ¨¡å‹ä½¿ç”¨](#6-æ¨¡å‹ä½¿ç”¨)
7. [å¸¸è§é—®é¢˜](#7-å¸¸è§é—®é¢˜)

---

## 1. ç¯å¢ƒå‡†å¤‡

### 1.1 å®‰è£…ä¾èµ–

é¡¹ç›®ä½¿ç”¨ `uv` è¿›è¡Œç¯å¢ƒå’Œä¾èµ–ç®¡ç†ï¼š

```bash
# é¦–æ¬¡ä½¿ç”¨ï¼šå®‰è£… uvï¼ˆå¦‚æœªå®‰è£…ï¼‰
curl -LsSf https://astral.sh/uv/install.sh | sh

# åŒæ­¥é¡¹ç›®ä¾èµ–
cd ImageClassifierModel
uv sync

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒï¼ˆå¯é€‰ï¼Œuv run ä¼šè‡ªåŠ¨å¤„ç†ï¼‰
source .venv/bin/activate  # macOS/Linux
# æˆ–
.venv\Scripts\activate     # Windows
```

### 1.2 éªŒè¯ç¯å¢ƒ

```bash
# æ£€æŸ¥ Python ç‰ˆæœ¬
uv run python --version  # åº”æ˜¾ç¤º Python 3.12+

# æ£€æŸ¥ PyTorch å®‰è£…
uv run python -c "import torch; print(f'PyTorch {torch.__version__}')"

# æ£€æŸ¥ GPU/MPS æ”¯æŒ
uv run python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}, MPS: {torch.backends.mps.is_available()}')"
```

---

## 2. æ•°æ®å‡†å¤‡

### 2.1 æ•°æ®é›†ç»“æ„

è®­ç»ƒæ•°æ®åº”æŒ‰ä»¥ä¸‹ç»“æ„ç»„ç»‡ï¼š

```
data/input/data1226/
â”œâ”€â”€ Failure/        # å¤±è´¥çŠ¶æ€æˆªå›¾
â”‚   â”œâ”€â”€ img_001.png
â”‚   â”œâ”€â”€ img_002.png
â”‚   â””â”€â”€ ...
â”œâ”€â”€ Loading/        # åŠ è½½çŠ¶æ€æˆªå›¾
â”‚   â”œâ”€â”€ img_001.png
â”‚   â””â”€â”€ ...
â””â”€â”€ Success/        # æˆåŠŸçŠ¶æ€æˆªå›¾
    â”œâ”€â”€ img_001.png
    â””â”€â”€ ...
```

### 2.2 æ•°æ®åˆ’åˆ†

å°†æ•°æ®åˆ†ä¸ºè®­ç»ƒé›†/éªŒè¯é›†/æµ‹è¯•é›†ï¼ˆ70%/15%/15%ï¼‰ï¼š

```bash
# ä½¿ç”¨å†…ç½®è„šæœ¬è‡ªåŠ¨åˆ’åˆ†æ•°æ®
uv run python -c "
from pathlib import Path
from src.data.split_data import split_dataset

split_dataset(
    input_dir=Path('data/input/data1226'),
    output_dir=Path('data/processed'),
    train_ratio=0.7,
    val_ratio=0.15,
    test_ratio=0.15,
    seed=42
)
print('âœ“ æ•°æ®é›†åˆ’åˆ†å®Œæˆï¼')
"
```

åˆ’åˆ†åçš„ç»“æ„ï¼š

```
data/processed/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ Failure/
â”‚   â”œâ”€â”€ Loading/
â”‚   â””â”€â”€ Success/
â”œâ”€â”€ val/
â”‚   â”œâ”€â”€ Failure/
â”‚   â”œâ”€â”€ Loading/
â”‚   â””â”€â”€ Success/
â””â”€â”€ test/
    â”œâ”€â”€ Failure/
    â”œâ”€â”€ Loading/
    â””â”€â”€ Success/
```

---

## 3. æ¨¡å‹è®­ç»ƒ

### 3.1 å¿«é€Ÿå¼€å§‹ï¼ˆæ¨èé…ç½®ï¼‰

ä½¿ç”¨é»˜è®¤é…ç½®è®­ç»ƒæ¨¡å‹ï¼š

```bash
uv run python scripts/train.py \
  --epochs 30 \
  --batch-size 16 \
  --lr 1e-3 \
  --pretrained
```

**å‚æ•°è¯´æ˜ï¼š**
- `--epochs 30`: è®­ç»ƒ 30 è½®
- `--batch-size 16`: æ‰¹æ¬¡å¤§å° 16
- `--lr 1e-3`: å­¦ä¹ ç‡ 0.001
- `--pretrained`: ä½¿ç”¨ ImageNet é¢„è®­ç»ƒæƒé‡

### 3.2 ä¸¤é˜¶æ®µè®­ç»ƒï¼ˆæ›´å¥½çš„æ•ˆæœï¼‰

å…ˆå†»ç»“ä¸»å¹²ç½‘ç»œè®­ç»ƒåˆ†ç±»å¤´ï¼Œå†å¾®è°ƒæ•´ä¸ªç½‘ç»œï¼š

```bash
uv run python scripts/train.py \
  --two-stage \
  --stage1-epochs 10 \
  --stage2-epochs 20 \
  --stage2-lr 1e-4 \
  --batch-size 16 \
  --pretrained
```

**è®­ç»ƒæµç¨‹ï¼š**
1. **é˜¶æ®µ 1** (10 epochs): å†»ç»“ MobileNetV2 ä¸»å¹²ï¼Œä»…è®­ç»ƒåˆ†ç±»å¤´
2. **é˜¶æ®µ 2** (20 epochs): è§£å†»éƒ¨åˆ†å±‚ï¼Œå¾®è°ƒæ•´ä¸ªç½‘ç»œ

### 3.3 å®Œæ•´å‚æ•°åˆ—è¡¨

```bash
uv run python scripts/train.py --help
```

**å¸¸ç”¨å‚æ•°ï¼š**

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `--epochs` | 30 | è®­ç»ƒè½®æ•° |
| `--batch-size` | 16 | æ‰¹æ¬¡å¤§å° |
| `--lr` | 1e-3 | å­¦ä¹ ç‡ |
| `--weight-decay` | 1e-4 | æƒé‡è¡°å‡ |
| `--dropout` | 0.3 | Dropout æ¯”ä¾‹ |
| `--img-size` | 224 | è¾“å…¥å›¾åƒå°ºå¯¸ |
| `--pretrained` | True | ä½¿ç”¨é¢„è®­ç»ƒæƒé‡ |
| `--patience` | 10 | æ—©åœè€å¿ƒå€¼ |
| `--device` | auto | è®¾å¤‡é€‰æ‹© (auto/mps/cuda/cpu) |
| `--data-root` | data/processed | æ•°æ®æ ¹ç›®å½• |

### 3.4 è®­ç»ƒè¾“å‡º

è®­ç»ƒè¿‡ç¨‹ä¼šè‡ªåŠ¨ä¿å­˜ï¼š

```
data/output/
â”œâ”€â”€ checkpoints/
â”‚   â””â”€â”€ best_model.pth           # æœ€ä½³æ¨¡å‹æ£€æŸ¥ç‚¹
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ training_YYYYMMDD_HHMMSS.log  # è®­ç»ƒæ—¥å¿—
â””â”€â”€ visualizations/
    â””â”€â”€ training_history.png     # è®­ç»ƒæ›²çº¿
```

### 3.5 ç›‘æ§è®­ç»ƒè¿‡ç¨‹

è®­ç»ƒæ—¶ä¼šå®æ—¶æ˜¾ç¤ºè¿›åº¦ï¼š

```
Epoch 10/30
Train Loss: 0.3245 | Train Acc: 89.23%
Val Loss:   0.2891 | Val Acc:   91.47%
Learning Rate: 0.001000

Early Stopping: 3/10 (æ— æå‡è½®æ•°)
Best Val Acc: 91.47% (Epoch 10)
```

---

## 4. æ¨¡å‹è¯„ä¼°

### 4.1 è¯„ä¼°æµ‹è¯•é›†

ä½¿ç”¨æµ‹è¯•é›†è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼š

```bash
uv run python scripts/evaluate.py \
  --checkpoint data/output/checkpoints/best_model.pth \
  --data-root data/processed \
  --output-dir data/output/metrics
```

### 4.2 è¯„ä¼°è¾“å‡º

è¯„ä¼°ä¼šç”Ÿæˆï¼š

1. **åˆ†ç±»æŠ¥å‘Š** (`classification_report.txt`):
```
              precision    recall  f1-score   support

     Failure     1.0000    0.8667    0.9286        15
     Loading     0.8235    1.0000    0.9032        14
     Success     1.0000    0.9286    0.9630        14

    accuracy                         0.9302        43
```

2. **æŒ‡æ ‡ JSON** (`test_metrics.json`): åŒ…å«è¯¦ç»†çš„æ€§èƒ½æŒ‡æ ‡

3. **å¯è§†åŒ–å›¾è¡¨**:
   - `test_confusion_matrix.png` - æ··æ·†çŸ©é˜µ
   - `test_per_class_metrics.png` - å„ç±»åˆ«æŒ‡æ ‡å¯¹æ¯”

### 4.3 æŸ¥çœ‹è¯„ä¼°ç»“æœ

```bash
# æŸ¥çœ‹æ–‡æœ¬æŠ¥å‘Š
cat data/output/metrics/classification_report.txt

# æŸ¥çœ‹ JSON æŒ‡æ ‡
cat data/output/metrics/test_metrics.json | python -m json.tool
```

---

## 5. æ¨¡å‹å¯¼å‡º

### 5.1 å¯¼å‡ºæ‰€æœ‰æ ¼å¼ï¼ˆæ¨èï¼‰

ä¸€é”®å¯¼å‡º ONNX å’Œ CoreML æ ¼å¼ï¼š

```bash
uv run python scripts/export.py \
  --checkpoint data/output/checkpoints/best_model.pth \
  --formats onnx coreml \
  --model-name screenshot_classifier \
  --output-dir data/output/exported_models
```

### 5.2 æŒ‰å¹³å°å¯¼å‡º

**å¯¼å‡º ONNXï¼ˆè·¨å¹³å°ï¼‰ï¼š**
```bash
uv run python scripts/export.py \
  --checkpoint data/output/checkpoints/best_model.pth \
  --formats onnx \
  --model-name screenshot_classifier
```

**å¯¼å‡º CoreMLï¼ˆiOS/macOSï¼‰ï¼š**
```bash
uv run python scripts/export.py \
  --checkpoint data/output/checkpoints/best_model.pth \
  --formats coreml \
  --model-name screenshot_classifier
```

**å¯¼å‡º TFLiteï¼ˆAndroidï¼‰ï¼š**

âš ï¸ TFLite éœ€è¦é¢å¤–ä¾èµ–ï¼Œæ¨èä½¿ç”¨åœ¨çº¿è½¬æ¢å·¥å…·ï¼š

**æ–¹æ³• 1: åœ¨çº¿è½¬æ¢ï¼ˆæ¨èï¼‰**
1. å…ˆå¯¼å‡º ONNX æ ¼å¼
2. è®¿é—® https://convertmodel.com/
3. ä¸Šä¼  `screenshot_classifier.onnx`
4. ä¸‹è½½ `.tflite` æ–‡ä»¶

**æ–¹æ³• 2: æœ¬åœ°è½¬æ¢**
```bash
# å®‰è£… TFLite ä¾èµ–
uv pip install tensorflow onnx onnx-tf

# å¯¼å‡º
uv run python scripts/export.py \
  --checkpoint data/output/checkpoints/best_model.pth \
  --formats tflite \
  --model-name screenshot_classifier
```

### 5.3 æ¨¡å‹é‡åŒ–ï¼ˆå‡å°ä½“ç§¯ï¼‰

```bash
uv run python scripts/export.py \
  --checkpoint data/output/checkpoints/best_model.pth \
  --formats coreml tflite \
  --quantize  # å¯ç”¨ FP16 é‡åŒ–
```

### 5.4 å¯¼å‡ºç»“æœ

```
data/output/exported_models/
â”œâ”€â”€ screenshot_classifier.onnx         # ONNX æ¨¡å‹
â”œâ”€â”€ screenshot_classifier.onnx.data    # ONNX å¤–éƒ¨æ•°æ®
â”œâ”€â”€ screenshot_classifier.mlpackage/   # CoreML æ¨¡å‹
â””â”€â”€ screenshot_classifier.tflite       # TFLite æ¨¡å‹ï¼ˆå¦‚æœå¯¼å‡ºï¼‰
```

---

## 6. æ¨¡å‹ä½¿ç”¨

### 6.1 æ‰¹é‡æ¨ç†ï¼ˆPythonï¼‰

å¯¹æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰å›¾ç‰‡è¿›è¡Œåˆ†ç±»ï¼š

```bash
uv run python scripts/batch_inference.py \
  --checkpoint data/output/checkpoints/best_model.pth \
  --input-dir /path/to/images \
  --output predictions.json
```

**å¤åˆ¶å›¾ç‰‡åˆ°åˆ†ç±»æ–‡ä»¶å¤¹ï¼š**
```bash
uv run python scripts/batch_inference.py \
  --checkpoint data/output/checkpoints/best_model.pth \
  --input-dir /path/to/images \
  --output predictions.json \
  --copy-to-folders \
  --output-dir data/output/classified_images
```

### 6.2 Python ä»£ç ç¤ºä¾‹

#### å•å¼ å›¾ç‰‡æ¨ç†

```python
import torch
from PIL import Image
from pathlib import Path

# 1. åŠ è½½æ¨¡å‹
from src.models.model_factory import load_model_from_checkpoint
from src.data.transforms import get_val_transforms
from src.utils.device import get_device

checkpoint_path = "data/output/checkpoints/best_model.pth"
model, checkpoint = load_model_from_checkpoint(checkpoint_path)
device = get_device()
model.to(device)
model.eval()

# 2. åŠ è½½å›¾ç‰‡
image_path = "test_image.png"
transform = get_val_transforms(img_size=224)
image = Image.open(image_path).convert('RGB')
image_tensor = transform(image).unsqueeze(0).to(device)

# 3. æ¨ç†
with torch.no_grad():
    outputs = model(image_tensor)
    probabilities = torch.nn.functional.softmax(outputs, dim=1)
    confidence, predicted = torch.max(probabilities, 1)

# 4. è§£æç»“æœ
class_names = ['Failure', 'Loading', 'Success']
predicted_class = class_names[predicted.item()]
confidence_score = confidence.item()

print(f"é¢„æµ‹ç±»åˆ«: {predicted_class}")
print(f"ç½®ä¿¡åº¦: {confidence_score:.2%}")
print(f"\næ‰€æœ‰ç±»åˆ«æ¦‚ç‡:")
for i, class_name in enumerate(class_names):
    prob = probabilities[0][i].item()
    print(f"  {class_name}: {prob:.2%}")
```

#### æ‰¹é‡æ¨ç†ï¼ˆä¼˜åŒ–ç‰ˆï¼‰

```python
import torch
from torch.utils.data import DataLoader
from pathlib import Path
from tqdm import tqdm

# åˆ›å»ºæ•°æ®é›†
class ImageDataset(torch.utils.data.Dataset):
    def __init__(self, image_paths, transform):
        self.image_paths = image_paths
        self.transform = transform

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        image = Image.open(self.image_paths[idx]).convert('RGB')
        return self.transform(image), str(self.image_paths[idx].name)

# åŠ è½½æ¨¡å‹å’Œæ•°æ®
model, _ = load_model_from_checkpoint("data/output/checkpoints/best_model.pth")
model.to(device).eval()

image_paths = list(Path("input_dir").glob("*.png"))
dataset = ImageDataset(image_paths, get_val_transforms())
dataloader = DataLoader(dataset, batch_size=32, num_workers=4)

# æ‰¹é‡æ¨ç†
results = {}
class_names = ['Failure', 'Loading', 'Success']

with torch.no_grad():
    for images, filenames in tqdm(dataloader):
        images = images.to(device)
        outputs = model(images)
        probs = torch.nn.functional.softmax(outputs, dim=1)

        for i, filename in enumerate(filenames):
            pred_idx = probs[i].argmax().item()
            results[filename] = {
                'class': class_names[pred_idx],
                'confidence': probs[i][pred_idx].item(),
                'probabilities': {
                    class_names[j]: probs[i][j].item()
                    for j in range(len(class_names))
                }
            }

# ä¿å­˜ç»“æœ
import json
with open('batch_results.json', 'w') as f:
    json.dump(results, f, indent=2)
```

### 6.3 ONNX æ¨ç†ï¼ˆè·¨å¹³å°ï¼‰

```python
import onnxruntime as ort
import numpy as np
from PIL import Image

# 1. åŠ è½½ ONNX æ¨¡å‹
session = ort.InferenceSession("data/output/exported_models/screenshot_classifier.onnx")

# 2. é¢„å¤„ç†å›¾ç‰‡
def preprocess_onnx(image_path):
    img = Image.open(image_path).convert('RGB')
    img = img.resize((224, 224))

    # è½¬æ¢ä¸ºæ•°ç»„å¹¶å½’ä¸€åŒ–
    img_array = np.array(img).astype(np.float32) / 255.0

    # ImageNet æ ‡å‡†åŒ–
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    img_array = (img_array - mean) / std

    # è°ƒæ•´ç»´åº¦: (H, W, C) -> (C, H, W) -> (1, C, H, W)
    img_array = np.transpose(img_array, (2, 0, 1))
    img_array = np.expand_dims(img_array, axis=0)

    return img_array

# 3. æ¨ç†
input_data = preprocess_onnx("test_image.png")
outputs = session.run(None, {'input': input_data})[0]

# 4. Softmax + è§£æ
from scipy.special import softmax
probs = softmax(outputs[0])

class_names = ['Failure', 'Loading', 'Success']
pred_idx = np.argmax(probs)

print(f"é¢„æµ‹: {class_names[pred_idx]} ({probs[pred_idx]:.2%})")
```

### 6.4 iOS é›†æˆï¼ˆCoreMLï¼‰

```swift
import CoreML
import Vision
import UIKit

class ScreenshotClassifier {
    private let model: screenshot_classifier

    init() throws {
        self.model = try screenshot_classifier(configuration: MLModelConfiguration())
    }

    func classify(image: UIImage, completion: @escaping (String, Double) -> Void) {
        // 1. è½¬æ¢ä¸º CVPixelBuffer
        guard let pixelBuffer = image.toCVPixelBuffer(size: CGSize(width: 224, height: 224)) else {
            return
        }

        // 2. åˆ›å»ºè¯·æ±‚
        guard let vnModel = try? VNCoreMLModel(for: model.model) else { return }

        let request = VNCoreMLRequest(model: vnModel) { request, error in
            guard let results = request.results as? [VNClassificationObservation],
                  let topResult = results.first else { return }

            completion(topResult.identifier, Double(topResult.confidence))
        }

        // 3. æ‰§è¡Œæ¨ç†
        let handler = VNImageRequestHandler(cvPixelBuffer: pixelBuffer, options: [:])
        try? handler.perform([request])
    }
}

// ä½¿ç”¨ç¤ºä¾‹
let classifier = try ScreenshotClassifier()
let image = UIImage(named: "test_screenshot")!

classifier.classify(image: image) { predictedClass, confidence in
    print("é¢„æµ‹: \(predictedClass), ç½®ä¿¡åº¦: \(confidence)")
}

// UIImage æ‰©å±•ï¼ˆè½¬æ¢ä¸º CVPixelBufferï¼‰
extension UIImage {
    func toCVPixelBuffer(size: CGSize) -> CVPixelBuffer? {
        let attrs = [
            kCVPixelBufferCGImageCompatibilityKey: kCFBooleanTrue,
            kCVPixelBufferCGBitmapContextCompatibilityKey: kCFBooleanTrue
        ] as CFDictionary

        var pixelBuffer: CVPixelBuffer?
        let status = CVPixelBufferCreate(
            kCFAllocatorDefault,
            Int(size.width),
            Int(size.height),
            kCVPixelFormatType_32ARGB,
            attrs,
            &pixelBuffer
        )

        guard status == kCVReturnSuccess, let buffer = pixelBuffer else {
            return nil
        }

        CVPixelBufferLockBaseAddress(buffer, CVPixelBufferLockFlags(rawValue: 0))
        let pixelData = CVPixelBufferGetBaseAddress(buffer)

        let colorSpace = CGColorSpaceCreateDeviceRGB()
        guard let context = CGContext(
            data: pixelData,
            width: Int(size.width),
            height: Int(size.height),
            bitsPerComponent: 8,
            bytesPerRow: CVPixelBufferGetBytesPerRow(buffer),
            space: colorSpace,
            bitmapInfo: CGImageAlphaInfo.noneSkipFirst.rawValue
        ) else {
            return nil
        }

        context.translateBy(x: 0, y: size.height)
        context.scaleBy(x: 1.0, y: -1.0)

        UIGraphicsPushContext(context)
        self.draw(in: CGRect(x: 0, y: 0, width: size.width, height: size.height))
        UIGraphicsPopContext()
        CVPixelBufferUnlockBaseAddress(buffer, CVPixelBufferLockFlags(rawValue: 0))

        return pixelBuffer
    }
}
```

### 6.5 Android é›†æˆï¼ˆTFLiteï¼‰

```kotlin
import org.tensorflow.lite.Interpreter
import android.graphics.Bitmap
import java.nio.ByteBuffer
import java.nio.ByteOrder

class ScreenshotClassifier(private val modelPath: String) {
    private val interpreter: Interpreter
    private val classNames = arrayOf("Failure", "Loading", "Success")

    init {
        interpreter = Interpreter(loadModelFile(modelPath))
    }

    fun classify(bitmap: Bitmap): Pair<String, Float> {
        // 1. é¢„å¤„ç†å›¾ç‰‡
        val inputBuffer = preprocessImage(bitmap)

        // 2. æ¨ç†
        val outputBuffer = Array(1) { FloatArray(3) }
        interpreter.run(inputBuffer, outputBuffer)

        // 3. è§£æç»“æœ
        val probabilities = outputBuffer[0]
        val maxIndex = probabilities.indices.maxByOrNull { probabilities[it] } ?: 0

        return Pair(classNames[maxIndex], probabilities[maxIndex])
    }

    private fun preprocessImage(bitmap: Bitmap): ByteBuffer {
        val inputSize = 224
        val byteBuffer = ByteBuffer.allocateDirect(1 * inputSize * inputSize * 3 * 4)
        byteBuffer.order(ByteOrder.nativeOrder())

        // Resize
        val scaledBitmap = Bitmap.createScaledBitmap(bitmap, inputSize, inputSize, true)

        // ImageNet æ ‡å‡†åŒ–å‚æ•°
        val mean = floatArrayOf(0.485f, 0.456f, 0.406f)
        val std = floatArrayOf(0.229f, 0.224f, 0.225f)

        // æå–åƒç´ å¹¶æ ‡å‡†åŒ–
        val intValues = IntArray(inputSize * inputSize)
        scaledBitmap.getPixels(intValues, 0, inputSize, 0, 0, inputSize, inputSize)

        for (pixel in intValues) {
            val r = ((pixel shr 16 and 0xFF) / 255.0f - mean[0]) / std[0]
            val g = ((pixel shr 8 and 0xFF) / 255.0f - mean[1]) / std[1]
            val b = ((pixel and 0xFF) / 255.0f - mean[2]) / std[2]

            byteBuffer.putFloat(r)
            byteBuffer.putFloat(g)
            byteBuffer.putFloat(b)
        }

        return byteBuffer
    }

    private fun loadModelFile(modelPath: String): java.nio.MappedByteBuffer {
        val fileDescriptor = assets.openFd(modelPath)
        val inputStream = java.io.FileInputStream(fileDescriptor.fileDescriptor)
        val fileChannel = inputStream.channel
        val startOffset = fileDescriptor.startOffset
        val declaredLength = fileDescriptor.declaredLength
        return fileChannel.map(java.nio.channels.FileChannel.MapMode.READ_ONLY, startOffset, declaredLength)
    }
}

// ä½¿ç”¨ç¤ºä¾‹
val classifier = ScreenshotClassifier("screenshot_classifier.tflite")
val bitmap = BitmapFactory.decodeResource(resources, R.drawable.test_image)

val (predictedClass, confidence) = classifier.classify(bitmap)
Log.d("Classifier", "é¢„æµ‹: $predictedClass, ç½®ä¿¡åº¦: ${confidence * 100}%")
```

---

## 7. å¸¸è§é—®é¢˜

### Q1: è®­ç»ƒæ—¶æ˜¾ç¤º "MPS backend out of memory"

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# å‡å°æ‰¹æ¬¡å¤§å°
uv run python scripts/train.py --batch-size 8

# æˆ–ä½¿ç”¨ CPU
uv run python scripts/train.py --device cpu
```

### Q2: å¦‚ä½•æ¢å¤ä¸­æ–­çš„è®­ç»ƒï¼Ÿ

è®­ç»ƒä¼šè‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹åˆ° `data/output/checkpoints/best_model.pth`ï¼Œå¦‚éœ€ç»§ç»­è®­ç»ƒéœ€è¦ä¿®æ”¹ä»£ç æ”¯æŒã€‚

### Q3: æ¨¡å‹å‡†ç¡®ç‡ä¸ç†æƒ³æ€ä¹ˆåŠï¼Ÿ

**ä¼˜åŒ–å»ºè®®ï¼š**

1. **å¢åŠ æ•°æ®é‡**ï¼šæ¯ä¸ªç±»åˆ«è‡³å°‘ 150+ æ ·æœ¬
2. **æ•°æ®å¢å¼º**ï¼šè®­ç»ƒæ—¶å·²è‡ªåŠ¨åº”ç”¨
3. **ä¸¤é˜¶æ®µè®­ç»ƒ**ï¼šä½¿ç”¨ `--two-stage` å‚æ•°
4. **è°ƒæ•´è¶…å‚æ•°**ï¼š
   ```bash
   uv run python scripts/train.py \
     --two-stage \
     --stage1-epochs 15 \
     --stage2-epochs 25 \
     --lr 5e-4 \
     --dropout 0.4
   ```

### Q4: å¦‚ä½•åœ¨æœåŠ¡å™¨ä¸Šè¿è¡Œæ¨ç†ï¼Ÿ

**ONNX Runtime éƒ¨ç½²ï¼š**

```python
# å®‰è£…
pip install onnxruntime  # CPU
# æˆ–
pip install onnxruntime-gpu  # GPU

# API ç¤ºä¾‹ (Flask)
from flask import Flask, request, jsonify
import onnxruntime as ort
import numpy as np
from PIL import Image
import io

app = Flask(__name__)
session = ort.InferenceSession("screenshot_classifier.onnx")
class_names = ['Failure', 'Loading', 'Success']

@app.route('/predict', methods=['POST'])
def predict():
    # æ¥æ”¶å›¾ç‰‡
    file = request.files['image']
    img = Image.open(io.BytesIO(file.read()))

    # é¢„å¤„ç†
    input_data = preprocess_onnx(img)  # ä½¿ç”¨å‰é¢çš„é¢„å¤„ç†å‡½æ•°

    # æ¨ç†
    outputs = session.run(None, {'input': input_data})[0]
    probs = softmax(outputs[0])

    # è¿”å›ç»“æœ
    pred_idx = np.argmax(probs)
    return jsonify({
        'class': class_names[pred_idx],
        'confidence': float(probs[pred_idx]),
        'probabilities': {
            class_names[i]: float(probs[i])
            for i in range(len(class_names))
        }
    })

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

### Q5: TFLite è½¬æ¢å¤±è´¥æ€ä¹ˆåŠï¼Ÿ

**æ¨èæ–¹æ¡ˆï¼š**

1. ä½¿ç”¨ ONNX æ ¼å¼ï¼ˆå…¼å®¹æ€§æœ€å¥½ï¼‰
2. åœ¨çº¿è½¬æ¢å·¥å…·ï¼šhttps://convertmodel.com/
3. ä½¿ç”¨ ONNX Runtime Mobileï¼ˆæ”¯æŒ Android/iOSï¼‰

### Q6: å¦‚ä½•æŸ¥çœ‹æ¨¡å‹è¯¦ç»†ä¿¡æ¯ï¼Ÿ

```bash
# PyTorch æ¨¡å‹
uv run python -c "
from src.models.model_factory import load_model_from_checkpoint
model, ckpt = load_model_from_checkpoint('data/output/checkpoints/best_model.pth')
print(model)
"

# ONNX æ¨¡å‹ï¼ˆä½¿ç”¨ Netronï¼‰
# è®¿é—® https://netron.app/
# ä¸Šä¼  screenshot_classifier.onnx æŸ¥çœ‹
```

---

## ğŸ“ æŠ€æœ¯æ”¯æŒ

- **é¡¹ç›®æ–‡æ¡£**: `CLAUDE.md`
- **è¯„ä¼°æŠ¥å‘Š**: `data/output/æ¨¡å‹è¯„ä¼°ä¸å¯¼å‡ºæ€»ç»“.md`
- **è®­ç»ƒæ—¥å¿—**: `data/output/logs/`

---

## ğŸ“ å¿«é€Ÿå‘½ä»¤å‚è€ƒ

```bash
# è®­ç»ƒæ¨¡å‹
uv run python scripts/train.py --two-stage --pretrained

# è¯„ä¼°æ¨¡å‹
uv run python scripts/evaluate.py --checkpoint data/output/checkpoints/best_model.pth

# å¯¼å‡ºæ¨¡å‹
uv run python scripts/export.py --checkpoint data/output/checkpoints/best_model.pth --formats onnx coreml

# æ‰¹é‡æ¨ç†
uv run python scripts/batch_inference.py --checkpoint data/output/checkpoints/best_model.pth --input-dir /path/to/images --copy-to-folders
```

---

**æœ€åæ›´æ–°**: 2025-12-27
**æ¨¡å‹ç‰ˆæœ¬**: v1.0
**ä½œè€…**: ImageClassifierModel Project
