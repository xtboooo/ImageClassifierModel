# æ¨¡å‹è¯„ä¼°ä¸å¯¼å‡ºæ€»ç»“

## ğŸ“Š æ¨¡å‹è¯„ä¼°ç»“æœ

### æ•´ä½“æ€§èƒ½
- **æµ‹è¯•é›†å‡†ç¡®ç‡**: **93.02%** (43ä¸ªæ ·æœ¬)
- **æœ€ä½³éªŒè¯å‡†ç¡®ç‡**: **95.24%**
- **å®å¹³å‡ F1-Score**: 0.9316
- **åŠ æƒå¹³å‡ F1-Score**: 0.9315

### å„ç±»åˆ«è¯¦ç»†æŒ‡æ ‡

| ç±»åˆ« | Precision | Recall | F1-Score | æ”¯æŒæ ·æœ¬ |
|------|-----------|--------|----------|----------|
| **Failure** | 100.00% | 86.67% | 92.86% | 15 |
| **Loading** | 82.35% | 100.00% | 90.32% | 14 |
| **Success** | 100.00% | 92.86% | 96.30% | 14 |

### æ··æ·†çŸ©é˜µåˆ†æ

```
              é¢„æµ‹
çœŸå®     Failure  Loading  Success
Failure     13       2        0      (86.67% æ­£ç¡®)
Loading      0      14        0      (100% æ­£ç¡®) âœ“
Success      0       1       13      (92.86% æ­£ç¡®)
```

**å…³é”®å‘ç°:**
- âœ… Loading ç±»è¯†åˆ«å®Œç¾(100% recall)
- âš ï¸ Failure ç±»: 2ä¸ªæ ·æœ¬è¢«è¯¯åˆ¤ä¸º Loading
- âš ï¸ Success ç±»: 1ä¸ªæ ·æœ¬è¢«è¯¯åˆ¤ä¸º Loading
- ğŸ’¡ å»ºè®®: å¯è€ƒè™‘å¢åŠ  Failure è®­ç»ƒæ ·æœ¬ä»¥è¿›ä¸€æ­¥æå‡ recall

---

## ğŸ“¦ å·²å¯¼å‡ºæ¨¡å‹

### 1. PyTorch æ¨¡å‹æ£€æŸ¥ç‚¹
- **è·¯å¾„**: `data/output/checkpoints/best_model.pth`
- **å¤§å°**: 29 MB
- **æ¶æ„**: MobileNetV2 (é¢„è®­ç»ƒ + å¾®è°ƒ)
- **ç”¨é€”**: PyTorch æ¨ç†ã€ç»§ç»­è®­ç»ƒã€æ¨¡å‹è½¬æ¢

### 2. ONNX æ¨¡å‹ âœ…
- **è·¯å¾„**: `data/output/exported_models/screenshot_classifier.onnx`
- **å¤§å°**: 11.6 MB
- **æ ¼å¼**: ONNX v14
- **è¾“å…¥**: `input` - [batch_size, 3, 224, 224] (NCHW)
- **è¾“å‡º**: `output` - [batch_size, 3]
- **ç”¨é€”**: è·¨å¹³å°éƒ¨ç½²ã€è½¬æ¢ä¸ºå…¶ä»–æ ¼å¼

**ä½¿ç”¨åœºæ™¯:**
- âœ… æœåŠ¡ç«¯æ¨ç†(ONNX Runtime)
- âœ… è½¬æ¢ä¸º TFLite/TensorRT
- âœ… ç§»åŠ¨ç«¯éƒ¨ç½²(ONNX Mobile)
- âœ… Web éƒ¨ç½²(ONNX.js)

### 3. CoreML æ¨¡å‹ âœ…
- **è·¯å¾„**: `data/output/exported_models/screenshot_classifier.mlpackage`
- **å¤§å°**: ~11 MB
- **æ ¼å¼**: CoreML v6
- **ç”¨é€”**: iOS/macOS åŸç”Ÿåº”ç”¨

**é›†æˆæ­¥éª¤(iOS/macOS):**
```swift
// 1. å°† .mlpackage æ‹–å…¥ Xcode é¡¹ç›®

// 2. å¯¼å…¥æ¨¡å‹
import CoreML
import Vision

// 3. ä½¿ç”¨æ¨¡å‹
let model = try screenshot_classifier(configuration: MLModelConfiguration())
let vnModel = try VNCoreMLModel(for: model.model)

// 4. åˆ›å»ºæ¨ç†è¯·æ±‚
let request = VNCoreMLRequest(model: vnModel) { request, error in
    guard let results = request.results as? [VNClassificationObservation] else { return }
    // å¤„ç†ç»“æœ
    for result in results {
        print("\(result.identifier): \(result.confidence)")
    }
}
```

### 4. TFLite æ¨¡å‹ (æ¨èæ–¹æ¡ˆ)

ç”±äº TFLite è½¬æ¢ä¾èµ–è¾ƒå¤š(éœ€è¦ tensorflowã€onnx-tf),æ¨èä»¥ä¸‹ä¸¤ç§æ–¹æ¡ˆ:

#### æ–¹æ¡ˆ A: åœ¨çº¿è½¬æ¢(æ¨è)
ä½¿ç”¨ Netron + ONNX-TFLite åœ¨çº¿å·¥å…·:
1. è®¿é—®: https://convertmodel.com/ æˆ– https://www.tensorflow.org/lite/models/convert
2. ä¸Šä¼  `screenshot_classifier.onnx`
3. é€‰æ‹©ç›®æ ‡æ ¼å¼: TensorFlow Lite
4. ä¸‹è½½è½¬æ¢åçš„ `.tflite` æ–‡ä»¶

#### æ–¹æ¡ˆ B: æœ¬åœ°è½¬æ¢
å¦‚éœ€æœ¬åœ°è½¬æ¢,å®‰è£…ä¾èµ–åè¿è¡Œ:

```bash
# å®‰è£…ä¾èµ–
uv pip install onnx onnx-tf tensorflow

# å¯¼å‡º TFLite
uv run python scripts/export.py \
  --checkpoint data/output/checkpoints/best_model.pth \
  --formats tflite \
  --model-name screenshot_classifier
```

#### Android é›†æˆç¤ºä¾‹

```kotlin
// 1. å°† .tflite æ–‡ä»¶æ”¾å…¥ assets ç›®å½•

// 2. æ·»åŠ ä¾èµ–(build.gradle)
implementation 'org.tensorflow:tensorflow-lite:2.13.0'
implementation 'org.tensorflow:tensorflow-lite-support:0.4.4'

// 3. åŠ è½½æ¨¡å‹
val tflite = Interpreter(loadModelFile())

// 4. é¢„å¤„ç†å›¾åƒ
val inputBuffer = ByteBuffer.allocateDirect(1 * 224 * 224 * 3 * 4)
// ... å¡«å……å›¾åƒæ•°æ®(å½’ä¸€åŒ–åˆ° [0, 1])

// 5. æ¨ç†
val outputBuffer = Array(1) { FloatArray(3) }
tflite.run(inputBuffer, outputBuffer)

// 6. è§£æç»“æœ
val classes = arrayOf("Failure", "Loading", "Success")
val probabilities = outputBuffer[0]
val maxIndex = probabilities.indices.maxByOrNull { probabilities[it] } ?: 0
println("é¢„æµ‹: ${classes[maxIndex]}, ç½®ä¿¡åº¦: ${probabilities[maxIndex]}")
```

---

## ğŸ¯ æ€§èƒ½å¯¹æ¯”

| æ ¼å¼ | å¤§å° | å¹³å° | æ¨ç†é€Ÿåº¦ | é›†æˆéš¾åº¦ |
|------|------|------|----------|----------|
| **PyTorch (.pth)** | 29 MB | Python | ä¸­ç­‰ | ç®€å• |
| **ONNX (.onnx)** | 11.6 MB | è·¨å¹³å° | å¿« | ä¸­ç­‰ |
| **CoreML (.mlpackage)** | 11 MB | iOS/macOS | å¾ˆå¿« | ç®€å• |
| **TFLite (.tflite)** | ~3-5 MB* | Android/ç§»åŠ¨ | å¾ˆå¿« | ä¸­ç­‰ |

*TFLite å¤§å°å–å†³äºæ˜¯å¦é‡åŒ–

---

## ğŸ“ æ¨¡å‹ä½¿ç”¨è¯´æ˜

### è¾“å…¥é¢„å¤„ç†
```python
from PIL import Image
import numpy as np

def preprocess_image(image_path, size=224):
    """
    é¢„å¤„ç†å›¾åƒç”¨äºæ¨¡å‹æ¨ç†

    Args:
        image_path: å›¾åƒè·¯å¾„
        size: ç›®æ ‡å°ºå¯¸(é»˜è®¤ 224)

    Returns:
        é¢„å¤„ç†åçš„å¼ é‡ [1, 3, 224, 224]
    """
    # 1. åŠ è½½å›¾åƒ
    img = Image.open(image_path).convert('RGB')

    # 2. Resize
    img = img.resize((size, size))

    # 3. è½¬æ¢ä¸ºæ•°ç»„å¹¶å½’ä¸€åŒ–
    img_array = np.array(img).astype(np.float32) / 255.0

    # 4. æ ‡å‡†åŒ–(ImageNetç»Ÿè®¡)
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    img_array = (img_array - mean) / std

    # 5. è°ƒæ•´ç»´åº¦ (H, W, C) -> (C, H, W)
    img_array = np.transpose(img_array, (2, 0, 1))

    # 6. æ·»åŠ  batch ç»´åº¦
    img_array = np.expand_dims(img_array, axis=0)

    return img_array
```

### è¾“å‡ºè§£æ
```python
def parse_output(logits, threshold=0.5):
    """
    è§£ææ¨¡å‹è¾“å‡º

    Args:
        logits: æ¨¡å‹è¾“å‡º [1, 3]
        threshold: ç½®ä¿¡åº¦é˜ˆå€¼

    Returns:
        (class_name, confidence, all_probabilities)
    """
    import torch.nn.functional as F

    # Softmax è½¬æ¢ä¸ºæ¦‚ç‡
    probs = F.softmax(torch.tensor(logits), dim=-1).numpy()[0]

    # ç±»åˆ«æ˜ å°„
    classes = ['Failure', 'Loading', 'Success']

    # è·å–é¢„æµ‹ç±»åˆ«
    pred_idx = probs.argmax()
    pred_class = classes[pred_idx]
    confidence = probs[pred_idx]

    # æ„å»ºå®Œæ•´ç»“æœ
    results = {
        'prediction': pred_class,
        'confidence': float(confidence),
        'probabilities': {
            classes[i]: float(probs[i])
            for i in range(len(classes))
        },
        'is_confident': confidence >= threshold
    }

    return results

# ä½¿ç”¨ç¤ºä¾‹
# output = model(input_tensor)
# result = parse_output(output)
# print(f"é¢„æµ‹: {result['prediction']} ({result['confidence']:.2%})")
```

---

## ğŸ“ˆ å¯è§†åŒ–ç»“æœ

æ¨¡å‹è¯„ä¼°ç”Ÿæˆçš„å¯è§†åŒ–å›¾è¡¨:

1. **æ··æ·†çŸ©é˜µ**: `data/output/visualizations/test_confusion_matrix.png`
2. **å„ç±»åˆ«æŒ‡æ ‡**: `data/output/visualizations/test_per_class_metrics.png`
3. **è®­ç»ƒå†å²**: `data/output/visualizations/training_history.png`

---

## ğŸš€ åç»­ä¼˜åŒ–å»ºè®®

### 1. æ•°æ®å¢å¼º
- å¢åŠ  Failure ç±»æ ·æœ¬æ•°é‡(å½“å‰95å¼ ,å»ºè®® 150+)
- åº”ç”¨æ›´å¼ºçš„æ•°æ®å¢å¼º(äº®åº¦ã€å¯¹æ¯”åº¦ã€æ¨¡ç³Š)
- æ”¶é›†æ›´å¤šè¾¹ç•Œ case

### 2. æ¨¡å‹ä¼˜åŒ–
- å°è¯• EfficientNet-Lite æ¶æ„(æ›´å°æ›´å¿«)
- åº”ç”¨é‡åŒ–(INT8)å‡å°æ¨¡å‹å¤§å°
- çŸ¥è¯†è’¸é¦åˆ°æ›´å°çš„å­¦ç”Ÿæ¨¡å‹

### 3. éƒ¨ç½²ä¼˜åŒ–
- æ·»åŠ æ¨ç†æ—¶é—´åŸºå‡†æµ‹è¯•
- å®ç°æ¨¡å‹ç¼“å­˜å’Œé¢„çƒ­
- æ·»åŠ æ¨ç†é”™è¯¯å¤„ç†å’Œå›é€€æœºåˆ¶

---

## ğŸ“š ç›¸å…³æ–‡ä»¶

```
data/output/
â”œâ”€â”€ checkpoints/
â”‚   â””â”€â”€ best_model.pth                     # PyTorch æ¨¡å‹æ£€æŸ¥ç‚¹
â”œâ”€â”€ exported_models/
â”‚   â”œâ”€â”€ screenshot_classifier.onnx          # ONNX æ¨¡å‹
â”‚   â”œâ”€â”€ screenshot_classifier.onnx.data     # ONNX å¤–éƒ¨æ•°æ®
â”‚   â””â”€â”€ screenshot_classifier.mlpackage/    # CoreML æ¨¡å‹
â”œâ”€â”€ metrics/
â”‚   â”œâ”€â”€ classification_report.txt           # åˆ†ç±»æŠ¥å‘Š
â”‚   â””â”€â”€ test_metrics.json                   # æµ‹è¯•æŒ‡æ ‡(JSON)
â”œâ”€â”€ visualizations/
â”‚   â”œâ”€â”€ test_confusion_matrix.png           # æ··æ·†çŸ©é˜µ
â”‚   â”œâ”€â”€ test_per_class_metrics.png          # å„ç±»åˆ«æŒ‡æ ‡
â”‚   â””â”€â”€ training_history.png                # è®­ç»ƒå†å²
â””â”€â”€ test_images_predictions.csv             # æµ‹è¯•é›†é¢„æµ‹ç»“æœ
```

---

ç”Ÿæˆæ—¶é—´: 2025-12-27
æ¨¡å‹ç‰ˆæœ¬: v1.0
